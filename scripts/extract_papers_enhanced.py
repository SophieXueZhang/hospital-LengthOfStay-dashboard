#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè®ºæ–‡æå–ï¼šä¼˜åŒ–æ ‡é¢˜ã€ä½œè€…ã€å¹´ä»½æå–ç®—æ³•
"""

import os
import sqlite3
import json
import openai
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
import time
import re
from datetime import datetime

# PDFå¤„ç†ç›¸å…³
import fitz  # PyMuPDF

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class EnhancedPaperExtractor:
    def __init__(self, papers_dir="data/papers", db_path="data/papers_rag.db"):
        self.papers_dir = Path(papers_dir)
        self.db_path = db_path
        self.client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # åˆ é™¤æ—§è¡¨é‡æ–°åˆ›å»º
        cursor.execute("DROP TABLE IF EXISTS paper_chunks")

        cursor.execute('''
        CREATE TABLE paper_chunks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            filename TEXT NOT NULL,
            title TEXT,
            authors TEXT,
            year INTEGER,
            content TEXT NOT NULL,
            chunk_text TEXT NOT NULL,
            chunk_index INTEGER,
            embedding TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        ''')

        conn.commit()
        conn.close()
        print("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

    def extract_txt_content(self, txt_path):
        """æå–TXTæ–‡ä»¶å†…å®¹"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()

            # ä½¿ç”¨å¢å¼ºç®—æ³•æå–å…ƒæ•°æ®
            title = self.extract_title_enhanced(content, txt_path.stem)
            authors = self.extract_authors_enhanced(content)
            year = self.extract_year_enhanced(content)

            return {
                'title': title,
                'authors': authors,
                'year': year,
                'content': content
            }
        except Exception as e:
            print(f"âŒ æå–TXTå¤±è´¥ {txt_path}: {e}")
            return None

    def extract_pdf_content(self, pdf_path):
        """æå–PDFå†…å®¹"""
        try:
            doc = fitz.open(pdf_path)
            content = ""

            # æå–æ–‡æœ¬
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                if page_text.strip():
                    content += f"\n=== ç¬¬{page_num+1}é¡µ ===\n{page_text}\n"

            doc.close()

            if len(content.strip()) > 100:
                print(f"  âœ… åŸç”Ÿæ–‡æœ¬æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")

                # ä½¿ç”¨å¢å¼ºç®—æ³•æå–å…ƒæ•°æ®
                title = self.extract_title_enhanced(content, pdf_path.stem)
                authors = self.extract_authors_enhanced(content)
                year = self.extract_year_enhanced(content)

                return {
                    'title': title,
                    'authors': authors,
                    'year': year,
                    'content': content
                }
            else:
                print(f"  âŒ åŸç”Ÿæå–æ–‡æœ¬ä¸è¶³ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
                return None

        except Exception as e:
            print(f"âŒ PDFå¤„ç†å¤±è´¥ {pdf_path}: {e}")
            return None

    def extract_title_enhanced(self, content, fallback_title):
        """å¢å¼ºç‰ˆæ ‡é¢˜æå–ç®—æ³•"""
        lines = [line.strip() for line in content.split('\n') if line.strip()]

        # ç­–ç•¥1: æŸ¥æ‰¾æ˜ç¡®çš„æ ‡é¢˜æ ‡è®°
        title_patterns = [
            r'(?i)^title[:\s]+(.+)',
            r'(?i)^article title[:\s]+(.+)',
            r'(?i)^paper title[:\s]+(.+)'
        ]

        for line in lines[:20]:
            for pattern in title_patterns:
                match = re.search(pattern, line)
                if match:
                    title = match.group(1).strip()
                    if 10 <= len(title) <= 200:
                        return title

        # ç­–ç•¥2: æŸ¥æ‰¾ç¬¬ä¸€é¡µçš„ä¸»è¦æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯æœ€å¤§çš„æ–‡æœ¬å—ï¼‰
        first_page_lines = []
        capturing = False

        for line in lines:
            if '=== ç¬¬1é¡µ ===' in line:
                capturing = True
                continue
            elif '=== ç¬¬2é¡µ ===' in line:
                break
            elif capturing:
                first_page_lines.append(line)

        # åœ¨ç¬¬ä¸€é¡µä¸­æŸ¥æ‰¾æ½œåœ¨æ ‡é¢˜
        for line in first_page_lines[:15]:
            # è·³è¿‡å¸¸è§çš„éæ ‡é¢˜è¡Œ
            if re.match(r'^(abstract|introduction|keywords|references|page \d+|vol\.|vol |journal|doi:|pmid:)', line, re.IGNORECASE):
                continue
            if re.match(r'^[A-Z\s]{5,}$', line):  # å…¨å¤§å†™å¯èƒ½æ˜¯æ ‡é¢˜
                continue
            if re.match(r'^\d+[\.\s]', line):  # æ•°å­—å¼€å¤´å¯èƒ½æ˜¯ç¼–å·
                continue

            # å¯èƒ½çš„æ ‡é¢˜ç‰¹å¾
            if (20 <= len(line) <= 200 and
                line.count(' ') >= 3 and  # è‡³å°‘4ä¸ªå•è¯
                not line.startswith('=') and
                ':' not in line[:20]):  # æ ‡é¢˜é€šå¸¸ä¸åœ¨å¼€å¤´æœ‰å†’å·
                return line

        # ç­–ç•¥3: ä»æ–‡ä»¶åä¼˜åŒ–æ ‡é¢˜
        enhanced_title = self.enhance_title_from_filename(fallback_title)
        return enhanced_title

    def enhance_title_from_filename(self, filename):
        """ä»æ–‡ä»¶åä¼˜åŒ–æ ‡é¢˜"""
        # ç§»é™¤å¸¸è§çš„æ–‡ä»¶æ ‡è¯†ç¬¦
        title = filename.replace('-', ' ').replace('_', ' ')

        # é¦–å­—æ¯å¤§å†™
        words = title.split()
        enhanced_words = []

        for word in words:
            if word.lower() in ['of', 'and', 'in', 'on', 'with', 'for', 'to', 'a', 'an', 'the']:
                enhanced_words.append(word.lower())
            else:
                enhanced_words.append(word.capitalize())

        return ' '.join(enhanced_words)

    def extract_authors_enhanced(self, content):
        """å¢å¼ºç‰ˆä½œè€…æå–ç®—æ³•"""
        lines = [line.strip() for line in content.split('\n') if line.strip()]

        # ç­–ç•¥1: æŸ¥æ‰¾æ˜ç¡®çš„ä½œè€…æ ‡è®°
        author_patterns = [
            r'(?i)^authors?[:\s]+(.+)',
            r'(?i)^by[:\s]+(.+)',
            r'(?i)^written by[:\s]+(.+)',
            r'(?i)^correspondent?[:\s]+(.+)'
        ]

        for line in lines[:30]:
            for pattern in author_patterns:
                match = re.search(pattern, line)
                if match:
                    authors = match.group(1).strip()
                    if len(authors) < 150:  # é¿å…æå–è¿‡é•¿çš„æ–‡æœ¬
                        return self.clean_authors(authors)

        # ç­–ç•¥2: æŸ¥æ‰¾å…¸å‹çš„ä½œè€…å§“åæ¨¡å¼
        first_page_lines = []
        capturing = False

        for line in lines:
            if '=== ç¬¬1é¡µ ===' in line:
                capturing = True
                continue
            elif '=== ç¬¬2é¡µ ===' in line:
                break
            elif capturing:
                first_page_lines.append(line)

        # åœ¨å‰å‡ è¡ŒæŸ¥æ‰¾ä½œè€…æ¨¡å¼
        author_name_patterns = [
            r'([A-Z][a-z]+ [A-Z]\. [A-Z][a-z]+)',  # John A. Smith
            r'([A-Z][a-z]+ [A-Z][a-z]+)',          # John Smith
            r'([A-Z]\. [A-Z][a-z]+)',              # J. Smith
            r'([A-Z][a-z]+, [A-Z]\.[A-Z]\.)',      # Smith, J.A.
        ]

        found_authors = []
        for line in first_page_lines[:10]:
            # è·³è¿‡æ˜æ˜¾ä¸æ˜¯ä½œè€…è¡Œçš„å†…å®¹
            if re.match(r'^(abstract|introduction|keywords|background)', line, re.IGNORECASE):
                break

            for pattern in author_name_patterns:
                matches = re.findall(pattern, line)
                for match in matches:
                    if len(match) >= 4 and match not in found_authors:
                        found_authors.append(match)

        if found_authors:
            if len(found_authors) == 1:
                return found_authors[0]
            else:
                return found_authors[0] + " et al."

        # ç­–ç•¥3: æŸ¥æ‰¾"et al."æ¨¡å¼
        for line in first_page_lines[:15]:
            if 'et al' in line.lower():
                # å°è¯•æå–ä¸»ä½œè€…
                words = line.split()
                for i, word in enumerate(words):
                    if 'et' in word.lower() and i > 0:
                        potential_author = words[i-1]
                        if len(potential_author) >= 3:
                            return potential_author + " et al."

        return "Unknown"

    def clean_authors(self, authors_text):
        """æ¸…ç†ä½œè€…æ–‡æœ¬"""
        # ç§»é™¤å¸¸è§çš„æ— å…³ä¿¡æ¯
        authors = authors_text.replace('\n', ' ').strip()

        # ç§»é™¤é‚®ç®±
        authors = re.sub(r'\S+@\S+', '', authors)

        # ç§»é™¤æœºæ„ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æ‹¬å·æˆ–é€—å·åï¼‰
        authors = re.sub(r'\([^)]+\)', '', authors)

        # é™åˆ¶é•¿åº¦
        if len(authors) > 100:
            # å¦‚æœå¤ªé•¿ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªä½œè€… + et al.
            first_author = authors.split(',')[0].split(' and ')[0].strip()
            authors = first_author + " et al."

        return authors.strip()

    def extract_year_enhanced(self, content):
        """å¢å¼ºç‰ˆå¹´ä»½æå–ç®—æ³•"""
        lines = [line.strip() for line in content.split('\n') if line.strip()]

        # ç­–ç•¥1: æŸ¥æ‰¾æ˜ç¡®çš„å¹´ä»½æ ‡è®°
        year_patterns = [
            r'(?i)year[:\s]+(\d{4})',
            r'(?i)published[:\s]+(\d{4})',
            r'(?i)copyright[:\s]+(\d{4})',
            r'(?i)\((\d{4})\)',
        ]

        for line in lines[:50]:
            for pattern in year_patterns:
                match = re.search(pattern, line)
                if match:
                    year = int(match.group(1))
                    if 1980 <= year <= 2030:
                        return year

        # ç­–ç•¥2: åœ¨æ ‡é¢˜é™„è¿‘æŸ¥æ‰¾å¹´ä»½
        first_page_lines = []
        capturing = False

        for line in lines:
            if '=== ç¬¬1é¡µ ===' in line:
                capturing = True
                continue
            elif '=== ç¬¬2é¡µ ===' in line:
                break
            elif capturing:
                first_page_lines.append(line)

        # æŸ¥æ‰¾å¹´ä»½æ¨¡å¼
        year_candidates = []
        for line in first_page_lines[:20]:
            # æŸ¥æ‰¾4ä½æ•°å¹´ä»½
            years = re.findall(r'\b(19[8-9]\d|20[0-3]\d)\b', line)
            for year_str in years:
                year = int(year_str)
                if 1980 <= year <= 2030:
                    year_candidates.append(year)

        if year_candidates:
            # è¿”å›æœ€å¸¸è§çš„å¹´ä»½ï¼Œæˆ–è€…æœ€æ–°çš„å¹´ä»½
            return max(year_candidates)

        # ç­–ç•¥3: ä»æœŸåˆŠä¿¡æ¯ä¸­æå–
        for line in first_page_lines:
            # æŸ¥æ‰¾æœŸåˆŠæ ¼å¼ä¸­çš„å¹´ä»½
            journal_patterns = [
                r'(\d{4});',
                r'(\d{4})\s*[;:]',
                r'Vol\.\s*\d+.*?(\d{4})',
                r'Volume\s*\d+.*?(\d{4})'
            ]

            for pattern in journal_patterns:
                match = re.search(pattern, line)
                if match:
                    year = int(match.group(1))
                    if 1980 <= year <= 2030:
                        return year

        return None

    def split_into_chunks(self, content, chunk_size=1000, overlap=200):
        """å°†å†…å®¹åˆ†å‰²æˆå—"""
        if not content or len(content) < chunk_size:
            return [content] if content else []

        chunks = []
        start = 0

        while start < len(content):
            end = start + chunk_size

            if end < len(content):
                period_pos = content.rfind('.', start, end)
                if period_pos > start + chunk_size // 2:
                    end = period_pos + 1

            chunk = content[start:end].strip()
            if chunk:
                chunks.append(chunk)

            start = end - overlap

        return chunks

    def get_embedding(self, text):
        """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
        try:
            response = self.client.embeddings.create(
                model="text-embedding-3-small",
                input=text[:8000]
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"  âš ï¸ è·å–embeddingå¤±è´¥: {e}")
            return None

    def save_to_database(self, paper_data, chunks):
        """ä¿å­˜è®ºæ–‡æ•°æ®åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            for i, chunk in enumerate(chunks):
                embedding = self.get_embedding(chunk)
                embedding_json = json.dumps(embedding) if embedding else None

                cursor.execute('''
                INSERT INTO paper_chunks
                (filename, title, authors, year, content, chunk_text, chunk_index, embedding)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    paper_data['filename'],
                    paper_data['title'],
                    paper_data['authors'],
                    paper_data['year'],
                    paper_data['content'],
                    chunk,
                    i,
                    embedding_json
                ))

                time.sleep(0.1)

            conn.commit()
            print(f"  âœ… ä¿å­˜æˆåŠŸ: {len(chunks)} ä¸ªæ–‡æœ¬å—")

        except Exception as e:
            print(f"  âŒ ä¿å­˜å¤±è´¥: {e}")
            conn.rollback()
        finally:
            conn.close()

    def process_all_papers(self):
        """å¤„ç†æ‰€æœ‰è®ºæ–‡æ–‡ä»¶"""
        if not self.papers_dir.exists():
            print(f"âŒ è®ºæ–‡ç›®å½•ä¸å­˜åœ¨: {self.papers_dir}")
            return

        txt_files = list(self.papers_dir.glob("*.txt"))
        pdf_files = list(self.papers_dir.glob("*.pdf"))

        print(f"å‘ç° {len(txt_files)} ä¸ªTXTæ–‡ä»¶ï¼Œ{len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

        total_processed = 0

        # å¤„ç†TXTæ–‡ä»¶
        print("\n=== å¤„ç†TXTæ–‡ä»¶ ===")
        for txt_file in txt_files:
            print(f"\nğŸ“„ å¤„ç†: {txt_file.name}")

            paper_data = self.extract_txt_content(txt_file)
            if paper_data:
                paper_data['filename'] = txt_file.name
                chunks = self.split_into_chunks(paper_data['content'])
                self.save_to_database(paper_data, chunks)
                total_processed += 1
                print(f"  æ ‡é¢˜: {paper_data['title']}")
                print(f"  ä½œè€…: {paper_data['authors']}")
                print(f"  å¹´ä»½: {paper_data['year']}")

        # å¤„ç†PDFæ–‡ä»¶
        print("\n=== å¤„ç†PDFæ–‡ä»¶ ===")
        for pdf_file in pdf_files:
            print(f"\nğŸ“„ å¤„ç†: {pdf_file.name}")

            paper_data = self.extract_pdf_content(pdf_file)
            if paper_data:
                paper_data['filename'] = pdf_file.name
                chunks = self.split_into_chunks(paper_data['content'])
                self.save_to_database(paper_data, chunks)
                total_processed += 1
                print(f"  æ ‡é¢˜: {paper_data['title']}")
                print(f"  ä½œè€…: {paper_data['authors']}")
                print(f"  å¹´ä»½: {paper_data['year']}")

        print(f"\nğŸ‰ æå–å®Œæˆï¼æ€»å…±å¤„ç†äº† {total_processed} ç¯‡è®ºæ–‡")

def main():
    print("å¼€å§‹å¢å¼ºç‰ˆè®ºæ–‡å…ƒæ•°æ®æå–...")

    extractor = EnhancedPaperExtractor()
    extractor.process_all_papers()

    # éªŒè¯ç»“æœ
    conn = sqlite3.connect("data/papers_rag.db")
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(DISTINCT filename) FROM paper_chunks")
    count = cursor.fetchone()[0]
    cursor.execute("SELECT COUNT(*) FROM paper_chunks")
    total_chunks = cursor.fetchone()[0]

    # ç»Ÿè®¡å…ƒæ•°æ®è´¨é‡
    cursor.execute("SELECT COUNT(*) FROM paper_chunks WHERE authors != 'Unknown' AND authors IS NOT NULL")
    has_authors = cursor.fetchone()[0]
    cursor.execute("SELECT COUNT(*) FROM paper_chunks WHERE year IS NOT NULL")
    has_years = cursor.fetchone()[0]

    conn.close()

    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
    print(f"   è®ºæ–‡æ•°é‡: {count}")
    print(f"   æ–‡æœ¬å—æ•°é‡: {total_chunks}")
    print(f"   æœ‰ä½œè€…ä¿¡æ¯: {has_authors}")
    print(f"   æœ‰å¹´ä»½ä¿¡æ¯: {has_years}")

if __name__ == "__main__":
    main()